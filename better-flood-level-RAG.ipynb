{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:15.359773Z",
     "start_time": "2025-08-28T21:38:15.227182Z"
    }
   },
   "source": [
    "from rich.console import Console\n",
    "from rich_theme_manager import ThemeManager\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "\"\"\"\n",
    "This is a RAG that:\n",
    " - gets mocked up data from the APSViz DB\n",
    " - uses a mini-LLM for the NLM.\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# load the secret DB credentials\n",
    "load_dotenv()\n",
    "\n",
    "# load the theme\n",
    "theme_dir = pathlib.Path(\"themes\")\n",
    "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
    "dark = theme_manager.get(\"dark\")\n",
    "\n",
    "# create a console with the dark theme\n",
    "console = Console(theme=dark)\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cpu\"  # set to \"cuda\" if you have GPU and torch.cuda.is_available()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:15.375305Z",
     "start_time": "2025-08-28T21:38:15.359773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_query(query):\n",
    "    \"\"\"\n",
    "    runs a query against the APSViz DB.\n",
    "\n",
    "    Note this notebook expects localhost to be connected to a postgres DB.\n",
    "    :param query:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Database connection parameters\n",
    "    connection = psycopg2.connect(dbname=\"apsviz\", user=os.getenv(\"PG_USER\"), password=os.getenv(\"PG_USER\"), host=\"localhost\", port=\"5432\")\n",
    "\n",
    "    results = None\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        try:\n",
    "            # Create a cursor object\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            # Execute an SQL query\n",
    "            cursor.execute(query)\n",
    "\n",
    "            # Fetch and print results\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "\n",
    "        finally:\n",
    "            # Close the cursor and connection\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "    return results[0][0]"
   ],
   "id": "fbb2f8dc220a642b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:15.663004Z",
     "start_time": "2025-08-28T21:38:15.380616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the SQL and get mocked up data from the DB.\n",
    "# note we are converting number values to feet.\n",
    "query = \"\"\"\n",
    "            SELECT json_agg(row_to_json(t))\n",
    "            FROM (\n",
    "                SELECT name, station_id, abbrev, lon, lat,\n",
    "                CASE WHEN nos_minor IS NOT NULL THEN (nos_minor * 3.28084) ELSE NULL END AS nos_minor,\n",
    "                CASE WHEN nos_moderate IS NOT NULL THEN (nos_moderate * 3.28084) ELSE NULL END AS nos_moderate,\n",
    "                CASE WHEN nos_major IS NOT NULL THEN (nos_major * 3.28084) ELSE NULL END AS nos_major,\n",
    "                CASE WHEN nws_minor IS NOT NULL THEN (nws_minor * 3.28084) ELSE NULL END AS nws_minor,\n",
    "                CASE WHEN nws_moderate IS NOT NULL THEN (nws_moderate * 3.28084) ELSE NULL END AS nws_moderate,\n",
    "                CASE WHEN nws_major IS NOT NULL THEN (nws_major * 3.28084) ELSE NULL END AS nws_major,\n",
    "                FLOOR(random() * 5 + 1)::INT AS current_level\n",
    "                FROM noaa_station_levels\n",
    "                ORDER BY name\n",
    "            ) t ;\n",
    "        \"\"\"\n",
    "# get the station data\n",
    "data = run_query(query)"
   ],
   "id": "be9631dd0df39d61",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:15.678456Z",
     "start_time": "2025-08-28T21:38:15.663534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_flood_stage(values):\n",
    "    \"\"\"\n",
    "    Gets the flood stage based on the station data\n",
    "    Note \"current_data\" is a random number (1 to 5) already generated in the data\n",
    "    \"\"\"\n",
    "    if ((values['nos_major'] and values['nos_major'] - values['current_level'] < 0) or (\n",
    "            values['nws_major'] and values['nws_major'] - values['current_level'] < 0)):\n",
    "        return 'major flooding'\n",
    "    elif ((values['nos_moderate'] and values['nos_moderate'] - values['current_level'] < 0) or (\n",
    "            values['nws_moderate'] and values['nws_moderate'] - values['current_level'] < 0)):\n",
    "        return 'moderate flooding'\n",
    "    elif ((values['nos_minor'] and values['nos_minor'] - values['current_level'] < 0) or (\n",
    "            values['nws_minor'] and values['nws_minor'] - values['current_level'] < 0)):\n",
    "        return 'minor flooding'\n",
    "    else:\n",
    "        return 'no flooding'"
   ],
   "id": "155d82df663ec32d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:15.727130Z",
     "start_time": "2025-08-28T21:38:15.685202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "get the data in an acceptable format\n",
    "\"\"\"\n",
    "\n",
    "docs = []\n",
    "metadata = []\n",
    "\n",
    "# for eact record returned from the DB\n",
    "for d in data:\n",
    "    # get the flood stage label for this station\n",
    "    flood_stage = get_flood_stage(d)\n",
    "\n",
    "    # create some data tags\n",
    "    tags = [str(d['station_id']), d['name'], flood_stage]\n",
    "\n",
    "    # get the data in a understandable format\n",
    "    text = f\"{d['name']}: {flood_stage} (tags: {','.join(tags)})\"\n",
    "\n",
    "    # save the\n",
    "    docs.append(text)\n",
    "    metadata.append(d)"
   ],
   "id": "f8115618892d7126",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:37.794511Z",
     "start_time": "2025-08-28T21:38:15.731202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "embed that data and create an embedding index\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# load the transformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# encode the data\n",
    "doc_embeddings = embedder.encode(docs, convert_to_numpy=True)\n",
    "\n",
    "# get the extents of the data\n",
    "dim = doc_embeddings.shape[1]\n",
    "\n",
    "# create an index\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# add the embeddings to the index\n",
    "index.add(doc_embeddings)"
   ],
   "id": "6f89601b6a6018c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\powen\\PycharmProjects\\AI-Sandbox\\venv-3.10.10\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:37.910201Z",
     "start_time": "2025-08-28T21:38:37.894953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_with_scores(query, top_k=2):\n",
    "    \"\"\"\n",
    "    gets the answer data with the scores\n",
    "\n",
    "    :param query:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # encode the query\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)  # (1, dim)\n",
    "\n",
    "    # get the scores and answer/data indexes\n",
    "    scores, idx = index.search(q_emb, top_k)  # returned L2 dists\n",
    "\n",
    "    # convert L2 -> similarity by negative distance (simple)\n",
    "    similarity = -scores[0].astype(float)  # length top_k\n",
    "\n",
    "    # init the results\n",
    "    results = []\n",
    "\n",
    "    # loop through the answers\n",
    "    for i in idx[0]:\n",
    "        # put away the result\n",
    "        results.append({\"text\": docs[i], \"metadata\": metadata[i]})\n",
    "\n",
    "    # get the embeddings of the answers\n",
    "    retrieved_embeddings = doc_embeddings[idx[0]]\n",
    "\n",
    "    return results, similarity, retrieved_embeddings"
   ],
   "id": "4bbb9699f2358beb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:37.966475Z",
     "start_time": "2025-08-28T21:38:37.943041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results, similarity, retrieved_embeddings = retrieve_with_scores(\"places with minor flooding\", 2)\n",
    "\n",
    "print('results:', results, 'similarity:', similarity)  # , 'embeddings:', retrieved_embeddings"
   ],
   "id": "557b2524c384653e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: [{'text': 'Cambridge: moderate flooding (tags: 8571892,Cambridge,moderate flooding)', 'metadata': {'name': 'Cambridge', 'station_id': 8571892, 'abbrev': 'Cambridg', 'lon': -76.061667, 'lat': 38.5725, 'nos_minor': 2.7407017073170734, 'nos_moderate': 3.71095012195122, 'nos_major': 4.94126512195122, 'nws_minor': 2.4806351219512197, 'nws_moderate': 2.980763170731708, 'nws_major': 3.4808912195121953, 'current_level': 3}}, {'text': 'Dahlgren: minor flooding (tags: 8635027,Dahlgren,minor flooding)', 'metadata': {'name': 'Dahlgren', 'station_id': 8635027, 'abbrev': 'Dahlgren', 'lon': -77.036597, 'lat': 38.319753, 'nos_minor': None, 'nos_moderate': None, 'nos_major': None, 'nws_minor': 2.5606556097561, 'nws_moderate': 4.061039756097562, 'nws_major': 6.071554512195122, 'current_level': 4}}] similarity: [-0.63787949 -0.74785548]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:40.071955Z",
     "start_time": "2025-08-28T21:38:37.988923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RagSequenceForGeneration, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer, BartTokenizer\n",
    "\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\").to(device)\n",
    "q_tok = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "ctx_tok = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "gen_tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ],
   "id": "3ccb33d59ef8b42a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:40.122487Z",
     "start_time": "2025-08-28T21:38:40.106925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fallback_concat_generate(query: str, contexts):\n",
    "    \"\"\"\n",
    "    dumps out the data when there has been an exception\n",
    "\n",
    "    :param query:\n",
    "    :param contexts:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create the model with the same generator weights\n",
    "    gen_model = model.generator.to(device)\n",
    "\n",
    "    # Build prompt: query + contexts joined (keeps provenance)\n",
    "    full_prompt = query + \" \" + \" \".join([f\"[DOC{i}] {c}\" for i, c in enumerate(contexts, start=1)])\n",
    "\n",
    "    # get the input tokens\n",
    "    inputs = gen_tok(full_prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # generate the output\n",
    "    out = gen_model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                             attention_mask=inputs[\"attention_mask\"],\n",
    "                             max_length=64,\n",
    "                             num_beams=2)\n",
    "\n",
    "    # get the answer\n",
    "    answer = gen_tok.batch_decode(out, skip_special_tokens=True)[0]\n",
    "\n",
    "    # return the results\n",
    "    return {\"query\": query, \"answer\": answer, \"sources\": contexts, \"note\": \"fallback_concat_used\"}"
   ],
   "id": "991ac825b04a1b23",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:40.154971Z",
     "start_time": "2025-08-28T21:38:40.123502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def rag_answer(query, top_k=2):\n",
    "    \"\"\"\n",
    "    gets the answer\n",
    "\n",
    "    :param query:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # get the results\n",
    "    retrieved_texts, sims, retrieved_embs = retrieve_with_scores(query, top_k=top_k)\n",
    "\n",
    "    # get a list of the text outputs\n",
    "    texts = [r[\"text\"] for r in retrieved_texts]\n",
    "\n",
    "    # get the length of the results\n",
    "    k = len(texts)\n",
    "\n",
    "    # make sure we have results\n",
    "    if k == 0:\n",
    "        return {\"query\": query, \"answer\": \"(no answers)\", \"sources\": []}\n",
    "\n",
    "    # tell the model how many docs per query\n",
    "    model.config.n_docs = k\n",
    "\n",
    "    # tokenize the question (DPRQuestion tokenizer)\n",
    "    q_inputs = q_tok(query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # tokenize the contexts (DPRContext tokenizer)\n",
    "    ctx_inputs = ctx_tok(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # build doc_scores tensor: shape (batch_size, n_docs) => here batch_size=1\n",
    "    # Use similarities computed from FAISS (converted to float). Make sure same device and dtype as\n",
    "    # else\n",
    "    doc_scores = torch.tensor(sims, dtype=torch.float32, device=device).unsqueeze(0)  # (1, k)\n",
    "\n",
    "    # debug prints to understand the results\n",
    "    # print(\"DEBUG SHAPES:\")\n",
    "    # print(\" q input_ids:\", q_inputs[\"input_ids\"].shape, \"dtype:\", q_inputs[\"input_ids\"].dtype)\n",
    "    # print(\" q attention_mask:\", q_inputs[\"attention_mask\"].shape)\n",
    "    # print(\" ctx input_ids:\", ctx_inputs[\"input_ids\"].shape, \"dtype:\", ctx_inputs[\"input_ids\"].dtype)\n",
    "    # print(\" ctx attention_mask:\", ctx_inputs[\"attention_mask\"].shape)\n",
    "    # print(\" model.config.n_docs:\", model.config.n_docs)\n",
    "    print(\" doc_scores:\", doc_scores.shape, \"values:\", doc_scores.tolist())\n",
    "\n",
    "    # final safety checks\n",
    "    assert \"input_ids\" in q_inputs and \"input_ids\" in ctx_inputs, \"missing tokenized ids\"\n",
    "    assert q_inputs[\"input_ids\"].dtype == torch.long and ctx_inputs[\"input_ids\"].dtype == torch.long\n",
    "    assert doc_scores.shape[1] == ctx_inputs[\"input_ids\"].shape[0], f\"doc_scores dim mismatch: {doc_scores.shape[1]} vs ctx count {ctx_inputs['input_ids'].shape[0]}\"\n",
    "\n",
    "    try:\n",
    "        # TODO: why doesnt this work?\n",
    "        #  get the RAG results\n",
    "        out_ids = model.generate(input_ids=q_inputs[\"input_ids\"],\n",
    "                                 attention_mask=q_inputs[\"attention_mask\"],\n",
    "                                 context_input_ids=ctx_inputs[\"input_ids\"],\n",
    "                                 context_attention_mask=ctx_inputs[\"attention_mask\"],\n",
    "                                 doc_scores=doc_scores, n_docs=k, max_length=64, num_beams=2, )\n",
    "\n",
    "        # grab the answer\n",
    "        answer = gen_tok.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        return {\"query\": query, \"answer\": answer, \"sources\": texts}\n",
    "\n",
    "    except AssertionError as e:\n",
    "        console.print(\"RAG generate assertion ERROR:\", e)\n",
    "\n",
    "        # fall back to concatenation method (guaranteed to work)\n",
    "        return fallback_concat_generate(query, texts)\n",
    "    except Exception as e:\n",
    "        console.print(\"RAG generate other ERROR:\", type(e).__name__, str(e))\n",
    "\n",
    "        # fall back to concatenation method (guaranteed to work)\n",
    "        return fallback_concat_generate(query, texts)"
   ],
   "id": "fe2c49e58c2b9e7f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T21:38:45.521474Z",
     "start_time": "2025-08-28T21:38:40.163486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts = ['places that have major flooding', 'places that have moderate flooding', 'places that have minor flooding', 'places that have no flooding']\n",
    "\n",
    "[print(rag_answer(x, 2)) for x in prompts]"
   ],
   "id": "c390a667bd61ed2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " doc_scores: torch.Size([1, 2]) values: [[-0.7302360534667969, -0.7324217557907104]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `\u001B[33mset_retriever\u001B[0m\u001B[1m(\u001B[0m\u001B[33m...\u001B[0m\u001B[1m)\u001B[0m` function.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `<span style=\"color: #808000; text-decoration-color: #808000\">set_retriever</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` function.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'places that have major flooding', 'answer': ' 8724580', 'sources': ['Cambridge: moderate flooding (tags: 8571892,Cambridge,moderate flooding)', 'Key West: major flooding (tags: 8724580,Key West,major flooding)'], 'note': 'fallback_concat_used'}\n",
      " doc_scores: torch.Size([1, 2]) values: [[-0.5711202621459961, -0.6826287508010864]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `\u001B[33mset_retriever\u001B[0m\u001B[1m(\u001B[0m\u001B[33m...\u001B[0m\u001B[1m)\u001B[0m` function.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `<span style=\"color: #808000; text-decoration-color: #808000\">set_retriever</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` function.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'places that have moderate flooding', 'answer': ' newport', 'sources': ['Cambridge: moderate flooding (tags: 8571892,Cambridge,moderate flooding)', 'Newport: moderate flooding (tags: 8452660,Newport,moderate flooding)'], 'note': 'fallback_concat_used'}\n",
      " doc_scores: torch.Size([1, 2]) values: [[-0.6690731644630432, -0.7583833932876587]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `\u001B[33mset_retriever\u001B[0m\u001B[1m(\u001B[0m\u001B[33m...\u001B[0m\u001B[1m)\u001B[0m` function.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `<span style=\"color: #808000; text-decoration-color: #808000\">set_retriever</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` function.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'places that have minor flooding', 'answer': ' newport', 'sources': ['Cambridge: moderate flooding (tags: 8571892,Cambridge,moderate flooding)', 'Newport: moderate flooding (tags: 8452660,Newport,moderate flooding)'], 'note': 'fallback_concat_used'}\n",
      " doc_scores: torch.Size([1, 2]) values: [[-0.8096615076065063, -0.8138640522956848]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `\u001B[33mset_retriever\u001B[0m\u001B[1m(\u001B[0m\u001B[33m...\u001B[0m\u001B[1m)\u001B[0m` function.\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RAG generate assertion ERROR: Make sure that `context_input_ids` are passed, if no `retriever` is set. \n",
       "Alternatively, you can set a retriever using the `<span style=\"color: #808000; text-decoration-color: #808000\">set_retriever</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"font-weight: bold\">)</span>` function.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'places that have no flooding', 'answer': ' 8461490', 'sources': ['Matagorda City: no flooding (tags: 8773146,Matagorda City,no flooding)', 'New London: no flooding (tags: 8461490,New London,no flooding)'], 'note': 'fallback_concat_used'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
